{"cells":[{"cell_type":"markdown","metadata":{"id":"mA17lSgSVba9"},"source":["# Transformer Network Application: Named-Entity Recognition\n","\n","Welcome to Week 4's first ungraded lab. In this notebook you'll explore one application of the transformer architecture that you built in the previous assignment.\n","\n","**After this assignment you'll be able to**:\n","\n","* Use tokenizers and pre-trained models from the HuggingFace Library.\n","* Fine-tune a pre-trained transformer model for Named-Entity Recognition"]},{"cell_type":"markdown","metadata":{"id":"TvKl6SNdVba_"},"source":["## Table of Contents\n","\n","- [Packages](#0)\n","- [1 - Named-Entity Recogniton to Process Resumes](#1)\n","    - [1.1 - Data Cleaning](#1-1)\n","    - [1.2 - Padding and Generating Tags](#1-2)\n","    - [1.3 - Tokenize and Align Labels with ü§ó Library](#1-3)\n","        - [Exercise 1 - tokenize_and_align_labels](#ex-1)\n","    - [1.4 - Optimization](#1-4)"]},{"cell_type":"markdown","metadata":{"id":"eTm8CNO_VbbA"},"source":["<a name='0'></a>\n","## Packages\n","\n","Run the following cell to load the packages you'll need."]},{"cell_type":"code","source":["# connect Google Colab with Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SSH3PcpE0Ex5","executionInfo":{"status":"ok","timestamp":1667897370566,"user_tz":-120,"elapsed":21344,"user":{"displayName":"–î–º–∏—Ç—Ä–∏–π –°—É—Ä–æ–≤–∏—Ü–∫–∏–π","userId":"04365192179387781824"}},"outputId":"f4974a4e-fd93-48fc-cb06-7d3281b2a82d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["cd drive/MyDrive/Deep Learning Specialization/5. Sequence Models/Week 4/Named Entity Recognition"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fWENXFvq0QMJ","executionInfo":{"status":"ok","timestamp":1667897372070,"user_tz":-120,"elapsed":617,"user":{"displayName":"–î–º–∏—Ç—Ä–∏–π –°—É—Ä–æ–≤–∏—Ü–∫–∏–π","userId":"04365192179387781824"}},"outputId":"8d4cf74c-6235-44da-db9e-3f3527b85067"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Deep Learning Specialization/5. Sequence Models/Week 4/Named Entity Recognition\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vP1SE4oMVbbA"},"outputs":[],"source":["import pandas as pd\n","import tensorflow as tf\n","import json\n","import random\n","import logging\n","import re"]},{"cell_type":"markdown","metadata":{"id":"bB1HzgDaVbbA"},"source":["<a name='1'></a>\n","## 1 - Named-Entity Recogniton to Process Resumes\n","\n","When faced with a large amount of unstructured text data, named-entity recognition (NER) can help you detect and classify important information in your dataset. For instance, in the running example \"Jane vists Africa in September\", NER would help you detect \"Jane\", \"Africa\", and \"September\" as named-entities and classify them as person, location, and time. \n","\n","* You will use a variation of the Transformer model you built in the last assignment to process a large dataset of resumes.\n","* You will find and classify relavent information such as the companies the applicant worked at, skills, type of degree, etc. "]},{"cell_type":"markdown","metadata":{"id":"p5oPbbleVbbB"},"source":["<a name='1-1'></a>\n","### 1.1 - Dataset Cleaning\n","\n","In this assignment you will optimize a Transformer model on a dataset of resumes. Take a look at how the data you will be working with are structured."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b3Yg8_vIVbbB"},"outputs":[],"source":["df_data = pd.read_json(\"ner.json\", lines=True)\n","df_data = df_data.drop(['extras'], axis=1)\n","df_data['content'] = df_data['content'].str.replace(\"\\n\", \" \")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"S1yHm01ZVbbB","executionInfo":{"status":"ok","timestamp":1667897376193,"user_tz":-120,"elapsed":15,"user":{"displayName":"–î–º–∏—Ç—Ä–∏–π –°—É—Ä–æ–≤–∏—Ü–∫–∏–π","userId":"04365192179387781824"}},"outputId":"a09e3e56-ae06-4320-b544-a4fdc6643a83"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                             content  \\\n","0  Abhishek Jha Application Development Associate...   \n","1  Afreen Jamadar Active member of IIIT Committee...   \n","2  Akhil Yadav Polemaina Hyderabad, Telangana - E...   \n","3  Alok Khandai Operational Analyst (SQL DBA) Eng...   \n","4  Ananya Chavan lecturer - oracle tutorials  Mum...   \n","\n","                                          annotation  \n","0  [{'label': ['Skills'], 'points': [{'start': 12...  \n","1  [{'label': ['Email Address'], 'points': [{'sta...  \n","2  [{'label': ['Skills'], 'points': [{'start': 37...  \n","3  [{'label': ['Skills'], 'points': [{'start': 80...  \n","4  [{'label': ['Degree'], 'points': [{'start': 20...  "],"text/html":["\n","  <div id=\"df-db8e3771-118c-4c67-8ae2-c77c9290503f\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>content</th>\n","      <th>annotation</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Abhishek Jha Application Development Associate...</td>\n","      <td>[{'label': ['Skills'], 'points': [{'start': 12...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Afreen Jamadar Active member of IIIT Committee...</td>\n","      <td>[{'label': ['Email Address'], 'points': [{'sta...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Akhil Yadav Polemaina Hyderabad, Telangana - E...</td>\n","      <td>[{'label': ['Skills'], 'points': [{'start': 37...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Alok Khandai Operational Analyst (SQL DBA) Eng...</td>\n","      <td>[{'label': ['Skills'], 'points': [{'start': 80...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Ananya Chavan lecturer - oracle tutorials  Mum...</td>\n","      <td>[{'label': ['Degree'], 'points': [{'start': 20...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-db8e3771-118c-4c67-8ae2-c77c9290503f')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-db8e3771-118c-4c67-8ae2-c77c9290503f button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-db8e3771-118c-4c67-8ae2-c77c9290503f');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":5}],"source":["df_data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oV0qILRYVbbC","executionInfo":{"status":"ok","timestamp":1667897376193,"user_tz":-120,"elapsed":14,"user":{"displayName":"–î–º–∏—Ç—Ä–∏–π –°—É—Ä–æ–≤–∏—Ü–∫–∏–π","userId":"04365192179387781824"}},"outputId":"1c957ba7-4184-4597-d64c-e4724e042ea1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'label': ['Skills'],\n","  'points': [{'start': 1295,\n","    'end': 1621,\n","    'text': '\\n‚Ä¢ Programming language: C, C++, Java\\n‚Ä¢ Oracle PeopleSoft\\n‚Ä¢ Internet Of Things\\n‚Ä¢ Machine Learning\\n‚Ä¢ Database Management System\\n‚Ä¢ Computer Networks\\n‚Ä¢ Operating System worked on: Linux, Windows, Mac\\n\\nNon - Technical Skills\\n\\n‚Ä¢ Honest and Hard-Working\\n‚Ä¢ Tolerant and Flexible to Different Situations\\n‚Ä¢ Polite and Calm\\n‚Ä¢ Team-Player'}]},\n"," {'label': ['Skills'],\n","  'points': [{'start': 993,\n","    'end': 1153,\n","    'text': 'C (Less than 1 year), Database (Less than 1 year), Database Management (Less than 1 year),\\nDatabase Management System (Less than 1 year), Java (Less than 1 year)'}]},\n"," {'label': ['College Name'],\n","  'points': [{'start': 939, 'end': 956, 'text': 'Kendriya Vidyalaya'}]},\n"," {'label': ['College Name'],\n","  'points': [{'start': 883, 'end': 904, 'text': 'Woodbine modern school'}]},\n"," {'label': ['Graduation Year'],\n","  'points': [{'start': 856, 'end': 860, 'text': '2017\\n'}]},\n"," {'label': ['College Name'],\n","  'points': [{'start': 771,\n","    'end': 813,\n","    'text': 'B.v.b college of engineering and technology'}]},\n"," {'label': ['Designation'],\n","  'points': [{'start': 727,\n","    'end': 769,\n","    'text': 'B.E in Information science and engineering\\n'}]},\n"," {'label': ['Companies worked at'],\n","  'points': [{'start': 407, 'end': 415, 'text': 'Accenture'}]},\n"," {'label': ['Designation'],\n","  'points': [{'start': 372,\n","    'end': 404,\n","    'text': 'Application Development Associate'}]},\n"," {'label': ['Email Address'],\n","  'points': [{'start': 95,\n","    'end': 145,\n","    'text': 'Indeed: indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a\\n'}]},\n"," {'label': ['Location'],\n","  'points': [{'start': 60, 'end': 68, 'text': 'Bengaluru'}]},\n"," {'label': ['Companies worked at'],\n","  'points': [{'start': 49, 'end': 57, 'text': 'Accenture'}]},\n"," {'label': ['Designation'],\n","  'points': [{'start': 13,\n","    'end': 45,\n","    'text': 'Application Development Associate'}]},\n"," {'label': ['Name'],\n","  'points': [{'start': 0, 'end': 11, 'text': 'Abhishek Jha'}]}]"]},"metadata":{},"execution_count":6}],"source":["df_data.iloc[0]['annotation']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jr8mKe9jVbbC"},"outputs":[],"source":["def mergeIntervals(intervals):\n","    sorted_by_lower_bound = sorted(intervals, key=lambda tup: tup[0])\n","    \n","    merged = []\n","\n","    for higher in sorted_by_lower_bound:\n","        if not merged:\n","            merged.append(higher)\n","        else:\n","            lower = merged[-1]\n","            if higher[0] <= lower[1]:\n","                if lower[2] is higher[2]:\n","                    upper_bound = max(lower[1], higher[1])\n","                    merged[-1] = (lower[0], upper_bound, lower[2])\n","                else:\n","                    if lower[1] > higher[1]:\n","                        merged[-1] = lower\n","                    else:\n","                        merged[-1] = (lower[0], higher[1], higher[2])\n","            else:\n","                merged.append(higher)\n","    return merged"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XkPrHB86VbbD"},"outputs":[],"source":["def get_entities(df):\n","    \n","    entities = []\n","    \n","    for i in range(len(df)):\n","        entity = []\n","    \n","        for annot in df['annotation'][i]:\n","            try:\n","                ent = annot['label'][0]\n","                start = annot['points'][0]['start']\n","                \n","                end = annot['points'][0]['end'] + 1\n","                \n","                entity.append((start, end, ent))\n","            except:\n","                pass\n","    \n","        entity = mergeIntervals(entity)\n","        entities.append(entity)\n","    \n","    return entities"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"l4VX1NYUVbbD","executionInfo":{"status":"ok","timestamp":1667897376691,"user_tz":-120,"elapsed":507,"user":{"displayName":"–î–º–∏—Ç—Ä–∏–π –°—É—Ä–æ–≤–∏—Ü–∫–∏–π","userId":"04365192179387781824"}},"outputId":"aa466c34-6163-4ba5-9574-ea9870a2dc9f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                             content  \\\n","0  Abhishek Jha Application Development Associate...   \n","1  Afreen Jamadar Active member of IIIT Committee...   \n","2  Akhil Yadav Polemaina Hyderabad, Telangana - E...   \n","3  Alok Khandai Operational Analyst (SQL DBA) Eng...   \n","4  Ananya Chavan lecturer - oracle tutorials  Mum...   \n","\n","                                          annotation  \\\n","0  [{'label': ['Skills'], 'points': [{'start': 12...   \n","1  [{'label': ['Email Address'], 'points': [{'sta...   \n","2  [{'label': ['Skills'], 'points': [{'start': 37...   \n","3  [{'label': ['Skills'], 'points': [{'start': 80...   \n","4  [{'label': ['Degree'], 'points': [{'start': 20...   \n","\n","                                            entities  \n","0  [(0, 12, Name), (13, 46, Designation), (49, 58...  \n","1  [(0, 14, Name), (62, 68, Location), (104, 148,...  \n","2  [(0, 21, Name), (22, 31, Location), (65, 117, ...  \n","3  [(0, 12, Name), (13, 51, Designation), (54, 60...  \n","4  [(0, 13, Name), (14, 22, Designation), (24, 41...  "],"text/html":["\n","  <div id=\"df-b3790b46-7d00-4d73-9c2a-f3d225264664\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>content</th>\n","      <th>annotation</th>\n","      <th>entities</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Abhishek Jha Application Development Associate...</td>\n","      <td>[{'label': ['Skills'], 'points': [{'start': 12...</td>\n","      <td>[(0, 12, Name), (13, 46, Designation), (49, 58...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Afreen Jamadar Active member of IIIT Committee...</td>\n","      <td>[{'label': ['Email Address'], 'points': [{'sta...</td>\n","      <td>[(0, 14, Name), (62, 68, Location), (104, 148,...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Akhil Yadav Polemaina Hyderabad, Telangana - E...</td>\n","      <td>[{'label': ['Skills'], 'points': [{'start': 37...</td>\n","      <td>[(0, 21, Name), (22, 31, Location), (65, 117, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Alok Khandai Operational Analyst (SQL DBA) Eng...</td>\n","      <td>[{'label': ['Skills'], 'points': [{'start': 80...</td>\n","      <td>[(0, 12, Name), (13, 51, Designation), (54, 60...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Ananya Chavan lecturer - oracle tutorials  Mum...</td>\n","      <td>[{'label': ['Degree'], 'points': [{'start': 20...</td>\n","      <td>[(0, 13, Name), (14, 22, Designation), (24, 41...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b3790b46-7d00-4d73-9c2a-f3d225264664')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-b3790b46-7d00-4d73-9c2a-f3d225264664 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-b3790b46-7d00-4d73-9c2a-f3d225264664');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":9}],"source":["df_data['entities'] = get_entities(df_data)\n","df_data.head()"]},{"cell_type":"code","source":["df_data.iloc[0]['entities']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ydgXXIZV8fmA","executionInfo":{"status":"ok","timestamp":1667897376692,"user_tz":-120,"elapsed":7,"user":{"displayName":"–î–º–∏—Ç—Ä–∏–π –°—É—Ä–æ–≤–∏—Ü–∫–∏–π","userId":"04365192179387781824"}},"outputId":"caf7285e-8130-473b-d3e0-dd4927d9c0bb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(0, 12, 'Name'),\n"," (13, 46, 'Designation'),\n"," (49, 58, 'Companies worked at'),\n"," (60, 69, 'Location'),\n"," (95, 146, 'Email Address'),\n"," (372, 405, 'Designation'),\n"," (407, 416, 'Companies worked at'),\n"," (727, 770, 'Designation'),\n"," (771, 814, 'College Name'),\n"," (856, 861, 'Graduation Year'),\n"," (883, 905, 'College Name'),\n"," (939, 957, 'College Name'),\n"," (993, 1154, 'Skills'),\n"," (1295, 1622, 'Skills')]"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"du5hhW4yVbbD"},"outputs":[],"source":["def convert_dataturks_to_spacy(dataturks_JSON_FilePath):\n","    try:\n","        training_data = []\n","        lines=[]\n","        with open(dataturks_JSON_FilePath, 'r') as f:\n","            lines = f.readlines()\n","\n","        for line in lines:\n","            data = json.loads(line)\n","            text = data['content'].replace(\"\\n\", \" \")\n","            entities = []\n","            data_annotations = data['annotation']\n","            if data_annotations is not None:\n","                for annotation in data_annotations:\n","                    #only a single point in text annotation.\n","                    point = annotation['points'][0]\n","                    labels = annotation['label']\n","                    # handle both list of labels or a single label.\n","                    if not isinstance(labels, list):\n","                        labels = [labels]\n","\n","                    for label in labels:\n","                        point_start = point['start']\n","                        point_end = point['end']\n","                        point_text = point['text']\n","                        \n","                        lstrip_diff = len(point_text) - len(point_text.lstrip())\n","                        rstrip_diff = len(point_text) - len(point_text.rstrip())\n","                        if lstrip_diff != 0:\n","                            point_start = point_start + lstrip_diff\n","                        if rstrip_diff != 0:\n","                            point_end = point_end - rstrip_diff\n","                        entities.append((point_start, point_end + 1 , label))\n","            training_data.append((text, {\"entities\" : entities}))\n","        return training_data\n","    except Exception as e:\n","        logging.exception(\"Unable to process \" + dataturks_JSON_FilePath + \"\\n\" + \"error = \" + str(e))\n","        return None\n","\n","def trim_entity_spans(data: list) -> list:\n","    \"\"\"Removes leading and trailing white spaces from entity spans.\n","\n","    Args:\n","        data (list): The data to be cleaned in spaCy JSON format.\n","\n","    Returns:\n","        list: The cleaned data.\n","    \"\"\"\n","    invalid_span_tokens = re.compile(r'\\s')\n","\n","    cleaned_data = []\n","    for text, annotations in data:\n","        entities = annotations['entities']\n","        valid_entities = []\n","        for start, end, label in entities:\n","            valid_start = start\n","            valid_end = end\n","            while valid_start < len(text) and invalid_span_tokens.match(\n","                    text[valid_start]):\n","                valid_start += 1\n","            while valid_end > 1 and invalid_span_tokens.match(\n","                    text[valid_end - 1]):\n","                valid_end -= 1\n","            valid_entities.append([valid_start, valid_end, label])\n","        cleaned_data.append([text, {'entities': valid_entities}])\n","    return cleaned_data  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k7egsaV_VbbD"},"outputs":[],"source":["data = trim_entity_spans(convert_dataturks_to_spacy(\"ner.json\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CpNpmQ6jVbbD"},"outputs":[],"source":["from tqdm.notebook import tqdm\n","def clean_dataset(data):\n","    cleanedDF = pd.DataFrame(columns=[\"setences_cleaned\"])\n","    sum1 = 0\n","    for i in tqdm(range(len(data))):\n","        start = 0\n","        emptyList = [\"Empty\"] * len(data[i][0].split())\n","        numberOfWords = 0\n","        lenOfString = len(data[i][0])\n","        strData = data[i][0]\n","        strDictData = data[i][1]\n","        lastIndexOfSpace = strData.rfind(' ')\n","        for i in range(lenOfString):\n","            if (strData[i]==\" \" and strData[i+1]!=\" \"):\n","                for k,v in strDictData.items():\n","                    for j in range(len(v)):\n","                        entList = v[len(v)-j-1]\n","                        if (start>=int(entList[0]) and i<=int(entList[1])):\n","                            emptyList[numberOfWords] = entList[2]\n","                            break\n","                        else:\n","                            continue\n","                start = i + 1  \n","                numberOfWords += 1\n","            if (i == lastIndexOfSpace):\n","                for j in range(len(v)):\n","                        entList = v[len(v)-j-1]\n","                        if (lastIndexOfSpace>=int(entList[0]) and lenOfString<=int(entList[1])):\n","                            emptyList[numberOfWords] = entList[2]\n","                            numberOfWords += 1\n","        cleanedDF = cleanedDF.append(pd.Series([emptyList],  index=cleanedDF.columns ), ignore_index=True )\n","        sum1 = sum1 + numberOfWords\n","    return cleanedDF"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["85005e53ecec4dc282a0a8658c76f091","f85bf860763c4af2b22e4ceb2bca7911","f9c37ba74f6049a9b07845add47ed2e5","44923acdbb514fb58a93c899bdd551d6","9c474c64143241a9b36b1a6189f55f6e","33cd488e46464ed1947461a0d7206a3a","8bc05dd5623847dda0d264b985ef8296","91174a6e7ea140c69272a4da25898bbe","5b5495898495428f8e13390be7408e1b","c1dab6053c134da9829facf934778f14","67ec9246b3074257942d566477817260"]},"id":"BtX35qm0VbbE","executionInfo":{"status":"ok","timestamp":1667897401192,"user_tz":-120,"elapsed":1747,"user":{"displayName":"–î–º–∏—Ç—Ä–∏–π –°—É—Ä–æ–≤–∏—Ü–∫–∏–π","userId":"04365192179387781824"}},"outputId":"8b63bd42-c9d7-4700-d658-3b52c235d327"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/220 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85005e53ecec4dc282a0a8658c76f091"}},"metadata":{}}],"source":["cleanedDF = clean_dataset(data)"]},{"cell_type":"markdown","metadata":{"id":"uM01cb-YVbbE"},"source":["Take a look at your cleaned dataset and the categories the named-entities are matched to, or 'tags'."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"T0-DJ4FxVbbE","executionInfo":{"status":"ok","timestamp":1667897404459,"user_tz":-120,"elapsed":337,"user":{"displayName":"–î–º–∏—Ç—Ä–∏–π –°—É—Ä–æ–≤–∏—Ü–∫–∏–π","userId":"04365192179387781824"}},"outputId":"9da78d7c-7f01-40a3-8a1b-9b16f8fa6cee"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                    setences_cleaned\n","0  [Name, Name, Designation, Designation, Designa...\n","1  [Name, Name, Empty, Empty, Empty, Empty, Empty...\n","2  [Name, Name, Name, Empty, Empty, Empty, Empty,...\n","3  [Name, Name, Designation, Designation, Designa...\n","4  [Name, Name, Designation, Empty, Companies wor..."],"text/html":["\n","  <div id=\"df-fe07a443-bc00-42b4-aa40-fdd09c6529ce\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>setences_cleaned</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[Name, Name, Designation, Designation, Designa...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[Name, Name, Empty, Empty, Empty, Empty, Empty...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[Name, Name, Name, Empty, Empty, Empty, Empty,...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>[Name, Name, Designation, Designation, Designa...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>[Name, Name, Designation, Empty, Companies wor...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fe07a443-bc00-42b4-aa40-fdd09c6529ce')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-fe07a443-bc00-42b4-aa40-fdd09c6529ce button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-fe07a443-bc00-42b4-aa40-fdd09c6529ce');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":15}],"source":["cleanedDF.head()"]},{"cell_type":"markdown","metadata":{"id":"7cgwDd8QVbbE"},"source":["<a name='1-2'></a>\n","### 1.2 - Padding and Generating Tags\n","\n","Now, it is time to generate a list of unique tags you will match the named-entities to."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KPue8NxAVbbE"},"outputs":[],"source":["unique_tags = set(cleanedDF['setences_cleaned'].explode().unique())#pd.unique(cleanedDF['setences_cleaned'])#set(tag for doc in cleanedDF['setences_cleaned'].values.tolist() for tag in doc)\n","tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n","id2tag = {id: tag for tag, id in tag2id.items()}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mlvmBO6hVbbE","executionInfo":{"status":"ok","timestamp":1667897437635,"user_tz":-120,"elapsed":407,"user":{"displayName":"–î–º–∏—Ç—Ä–∏–π –°—É—Ä–æ–≤–∏—Ü–∫–∏–π","userId":"04365192179387781824"}},"outputId":"35e67e35-56bd-4f61-df65-0d29fe5eda60"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'College Name',\n"," 'Companies worked at',\n"," 'Degree',\n"," 'Designation',\n"," 'Email Address',\n"," 'Empty',\n"," 'Graduation Year',\n"," 'Location',\n"," 'Name',\n"," 'Skills',\n"," 'UNKNOWN',\n"," 'Years of Experience'}"]},"metadata":{},"execution_count":17}],"source":["unique_tags"]},{"cell_type":"markdown","metadata":{"id":"3Gx3OEvyVbbE"},"source":["Next, you will create an array of tags from your cleaned dataset. Oftentimes your input sequence will exceed the maximum length of a sequence your network can process. In this case, your sequence will be cut off, and you need to append zeroes onto the end of the shortened sequences using this [Keras padding API](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2iVXqKmBVbbE"},"outputs":[],"source":["from tensorflow.keras.preprocessing.sequence import pad_sequences"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zQ3N437MVbbE"},"outputs":[],"source":["MAX_LEN = 512\n","labels = cleanedDF['setences_cleaned'].values.tolist()\n","\n","tags = pad_sequences([[tag2id.get(l) for l in lab] for lab in labels],\n","                     maxlen=MAX_LEN, value=tag2id[\"Empty\"], padding=\"post\",\n","                     dtype=\"long\", truncating=\"post\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZOZgU_YBVbbF","executionInfo":{"status":"ok","timestamp":1667897640178,"user_tz":-120,"elapsed":254,"user":{"displayName":"–î–º–∏—Ç—Ä–∏–π –°—É—Ä–æ–≤–∏—Ü–∫–∏–π","userId":"04365192179387781824"}},"outputId":"078c14ab-1b4f-47bf-a824-d96dad4dee21"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 0,  0, 11, ...,  1,  1,  1],\n","       [ 0,  0,  1, ...,  1,  1,  1],\n","       [ 0,  0,  0, ...,  1, 10,  1],\n","       ...,\n","       [ 0,  0, 11, ...,  1,  1,  1],\n","       [ 0,  0, 11, ...,  1,  1,  1],\n","       [ 0,  0, 11, ...,  1,  1,  1]])"]},"metadata":{},"execution_count":20}],"source":["tags"]},{"cell_type":"markdown","metadata":{"id":"EbX7BRVsVbbF"},"source":["<a name='1-3'></a>\n","### 1.3 - Tokenize and Align Labels with ü§ó Library\n","\n","Before feeding the texts to a Transformer model, you will need to tokenize your input using a [ü§ó Transformer tokenizer](https://huggingface.co/transformers/main_classes/tokenizer.html). It is crucial that the tokenizer you use must match the Transformer model type you are using! In this exercise, you will use the ü§ó [DistilBERT fast tokenizer](https://huggingface.co/transformers/model_doc/distilbert.html), which standardizes the length of your sequence to 512 and pads with zeros. Notice this matches the maximu length you used when creating tags. "]},{"cell_type":"code","source":["pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tqt9k5gYeGzb","executionInfo":{"status":"ok","timestamp":1667898067886,"user_tz":-120,"elapsed":12817,"user":{"displayName":"–î–º–∏—Ç—Ä–∏–π –°—É—Ä–æ–≤–∏—Ü–∫–∏–π","userId":"04365192179387781824"}},"outputId":"7eb94e1c-9a0e-492a-e5ca-9e88ff7b6baa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.5 MB 5.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Collecting huggingface-hub<1.0,>=0.10.0\n","  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 163 kB 56.4 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.6 MB 43.0 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.10.1 tokenizers-0.13.2 transformers-4.24.0\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SLRRHVTKVbbF","executionInfo":{"status":"ok","timestamp":1667898071052,"user_tz":-120,"elapsed":1025,"user":{"displayName":"–î–º–∏—Ç—Ä–∏–π –°—É—Ä–æ–≤–∏—Ü–∫–∏–π","userId":"04365192179387781824"}},"outputId":"706e1a9b-cc96-4af2-cda2-e3a90181a0fe"},"outputs":[{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["from transformers import DistilBertTokenizerFast #, TFDistilBertModel\n","tokenizer = DistilBertTokenizerFast.from_pretrained('tokenizer/')"]},{"cell_type":"markdown","metadata":{"id":"juzv9IjQVbbF"},"source":["Transformer models are often trained by tokenizers that split words into subwords. For instance, the word 'Africa' might get split into multiple subtokens. This can create some misalignment between the list of tags for the dataset and the list of labels generated by the tokenizer, since the tokenizer can split one word into several, or add special tokens. Before processing, it is important that you align the lists of tags and the list of labels generated by the selected tokenizer with a `tokenize_and_align_labels()` function.\n","\n","<a name='ex-1'></a>\n","### Exercise 1 - tokenize_and_align_labels\n","\n","Implement `tokenize_and_align_labels()`. The function should perform the following:\n","* The tokenizer cuts sequences that exceed the maximum size allowed by your model with the parameter `truncation=True`\n","* Aligns the list of tags and labels with the tokenizer `word_ids` method returns a list that maps the subtokens to the original word in the sentence and special tokens to `None`. \n","* Set the labels of all the special tokens (`None`) to -100 to prevent them from affecting the loss function. \n","* Label of the first subtoken of a word and set the label for the following subtokens to -100. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LUvtOylGVbbF"},"outputs":[],"source":["label_all_tokens = True\n","def tokenize_and_align_labels(tokenizer, examples, tags):\n","    tokenized_inputs = tokenizer(examples, truncation=True, is_split_into_words=False, padding='max_length', max_length=512)\n","    labels = []\n","    for i, label in enumerate(tags):\n","        word_ids = tokenized_inputs.word_ids(batch_index=i)\n","        previous_word_idx = None\n","        label_ids = []\n","        for word_idx in word_ids:\n","            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n","            # ignored in the loss function.\n","            if word_idx is None:\n","                label_ids.append(-100)\n","            # We set the label for the first token of each word.\n","            elif word_idx != previous_word_idx:\n","                label_ids.append(label[word_idx])\n","            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n","            # the label_all_tokens flag.\n","            else:\n","                label_ids.append(label[word_idx] if label_all_tokens else -100)\n","            previous_word_idx = word_idx\n","\n","        labels.append(label_ids)\n","\n","    tokenized_inputs[\"labels\"] = labels\n","    return tokenized_inputs"]},{"cell_type":"markdown","metadata":{"id":"GS9hBNJLVbbF"},"source":["Now that you have tokenized inputs, you can create train and test datasets!"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":425},"id":"PI_Ow60aVbbF","executionInfo":{"status":"error","timestamp":1667909292375,"user_tz":-120,"elapsed":254,"user":{"displayName":"–î–º–∏—Ç—Ä–∏–π –°—É—Ä–æ–≤–∏—Ü–∫–∏–π","userId":"04365192179387781824"}},"outputId":"d74b263c-fe73-490b-e8de-00ffb4144b2d"},"outputs":[{"output_type":"error","ename":"Exception","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-e42930bacfa3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize_and_align_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m train_dataset = tf.data.Dataset.from_tensor_slices((\n\u001b[1;32m      3\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m ))\n","\u001b[0;32m<ipython-input-24-6a1e9eabea79>\u001b[0m in \u001b[0;36mtokenize_and_align_labels\u001b[0;34m(tokenizer, examples, tags)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlabel_all_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokenize_and_align_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtokenized_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_split_into_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max_length'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2486\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_target_context_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2487\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_input_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2488\u001b[0;31m             \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2489\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext_target\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2490\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_target_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2589\u001b[0m                 \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2590\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2591\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2592\u001b[0m             )\n\u001b[1;32m   2593\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2780\u001b[0m             \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2781\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2782\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2783\u001b[0m         )\n\u001b[1;32m   2784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m             \u001b[0mis_pretokenized\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_split_into_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m         )\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mException\u001b[0m: WordPiece error: Missing [UNK] token from the vocabulary"]}],"source":["test = tokenize_and_align_labels(tokenizer, df_data['content'].values.tolist(), tags)\n","train_dataset = tf.data.Dataset.from_tensor_slices((\n","    test['input_ids'],\n","    test['labels']\n","))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":169},"id":"ak_HytHAVbbF","executionInfo":{"status":"error","timestamp":1667909027424,"user_tz":-120,"elapsed":2,"user":{"displayName":"–î–º–∏—Ç—Ä–∏–π –°—É—Ä–æ–≤–∏—Ü–∫–∏–π","userId":"04365192179387781824"}},"outputId":"5c369002-5ecf-4152-d645-ac0721093163"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-c621f4453844>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"]}],"source":["test['labels'][0]"]},{"cell_type":"markdown","metadata":{"id":"ELeALN6kVbbF"},"source":["<a name='1-4'></a>\n","### 1.4 - Optimization\n","\n","Fantastic! Now you can finally feed your data into into a pretrained ü§ó model. You will optimize a DistilBERT model, which matches the tokenizer you used to preprocess your data. Try playing around with the different hyperparamters to improve your results!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PpwLhsOqVbbF","outputId":"0dbd689b-6e6f-479f-f7ec-de9285534b35"},"outputs":[{"name":"stderr","output_type":"stream","text":["All model checkpoint layers were used when initializing TFDistilBertForTokenClassification.\n","\n","All the layers of TFDistilBertForTokenClassification were initialized from the model checkpoint at model/.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForTokenClassification for predictions without further training.\n"]}],"source":["from transformers import TFDistilBertForTokenClassification\n","\n","model = TFDistilBertForTokenClassification.from_pretrained('model/', num_labels=len(unique_tags))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3MQTPMEbVbbF"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"58UYrGfeVbbF"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z_1EDjgOVbbF","outputId":"0a442d58-9ba7-4c0d-c336-f55e5914d9f0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/3\n","WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fcd80166ee8>> and will run it as-is.\n","Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n","Cause: <cyfunction Socket.send at 0x7fcd853ee430> is not a module, class, method, function, traceback, frame, or code object\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fcd80166ee8>> and will run it as-is.\n","Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n","Cause: <cyfunction Socket.send at 0x7fcd853ee430> is not a module, class, method, function, traceback, frame, or code object\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","\n","WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n","WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","14/14 [==============================] - 4s 299ms/step - loss: 1.8904 - accuracy: 0.5558\n","Epoch 2/3\n","14/14 [==============================] - 4s 300ms/step - loss: 0.6804 - accuracy: 0.7537\n","Epoch 3/3\n","14/14 [==============================] - 4s 300ms/step - loss: 0.5309 - accuracy: 0.7537\n"]},{"data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7fcbc4474208>"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n","model.compile(optimizer=optimizer, loss=model.compute_loss, metrics=['accuracy']) # can also use any keras loss fn\n","model.fit(train_dataset.shuffle(1000).batch(16),\n","          epochs=3, \n","          batch_size=16)"]},{"cell_type":"markdown","metadata":{"id":"loIx0wb0VbbG"},"source":["### Congratulations!\n","\n","#### Here's what you should remember\n","\n","- Named-entity recognition (NER) detects and classifies named-entities, and can help process resumes, customer reviews, browsing histories, etc. \n","- You must preprocess text data with the corresponding tokenizer to the pretrained model before feeding your input into your Transformer model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ah0TSsT7VbbG"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"colab":{"provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"85005e53ecec4dc282a0a8658c76f091":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f85bf860763c4af2b22e4ceb2bca7911","IPY_MODEL_f9c37ba74f6049a9b07845add47ed2e5","IPY_MODEL_44923acdbb514fb58a93c899bdd551d6"],"layout":"IPY_MODEL_9c474c64143241a9b36b1a6189f55f6e"}},"f85bf860763c4af2b22e4ceb2bca7911":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_33cd488e46464ed1947461a0d7206a3a","placeholder":"‚Äã","style":"IPY_MODEL_8bc05dd5623847dda0d264b985ef8296","value":"100%"}},"f9c37ba74f6049a9b07845add47ed2e5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_91174a6e7ea140c69272a4da25898bbe","max":220,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5b5495898495428f8e13390be7408e1b","value":220}},"44923acdbb514fb58a93c899bdd551d6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c1dab6053c134da9829facf934778f14","placeholder":"‚Äã","style":"IPY_MODEL_67ec9246b3074257942d566477817260","value":" 220/220 [00:01&lt;00:00, 167.04it/s]"}},"9c474c64143241a9b36b1a6189f55f6e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"33cd488e46464ed1947461a0d7206a3a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8bc05dd5623847dda0d264b985ef8296":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"91174a6e7ea140c69272a4da25898bbe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b5495898495428f8e13390be7408e1b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c1dab6053c134da9829facf934778f14":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"67ec9246b3074257942d566477817260":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}